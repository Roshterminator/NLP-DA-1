{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca81bb83",
   "metadata": {},
   "source": [
    "### Name: Roshan Philip Jacob\n",
    "### Reg No: 20BCE1200\n",
    "### NLP DA-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde127f",
   "metadata": {},
   "source": [
    "Q1 ) Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following. Install relevant Packages and Libraries\n",
    "\n",
    "    • Explore Brown Corpus and find the size, tokens, categories,\n",
    "    • Find the size of word tokens?\n",
    "    • Find the size of word types?\n",
    "    • Find the size of the category “government”\n",
    "    • List the most frequent tokens\n",
    "    • Count the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a987a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac415f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368c975",
   "metadata": {},
   "source": [
    "#### Size of Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87690791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Brown Corpus:  1161192\n"
     ]
    }
   ],
   "source": [
    "corpus_size=len(brown.words())\n",
    "print(\"Size of Brown Corpus: \",corpus_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba70e18",
   "metadata": {},
   "source": [
    "#### Tokens in Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98074c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 1439319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'Grand/jj-tl',\n",
       " 'Jury/nn-tl',\n",
       " 'said/vbd',\n",
       " 'Friday/nr',\n",
       " 'an/at',\n",
       " 'investigation/nn',\n",
       " 'of/in',\n",
       " \"Atlanta's/np\",\n",
       " '$',\n",
       " 'recent/jj',\n",
       " 'primary/nn',\n",
       " 'election/nn',\n",
       " 'produced/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'no/at',\n",
       " 'evidence/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'that/cs',\n",
       " 'any/dti',\n",
       " 'irregularities/nns',\n",
       " 'took/vbd',\n",
       " 'place/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'further/rbr',\n",
       " 'said/vbd',\n",
       " 'in/in',\n",
       " 'term-end/nn',\n",
       " 'presentments/nns',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'City/nn-tl',\n",
       " 'Executive/jj-tl',\n",
       " 'Committee/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'which/wdt',\n",
       " 'had/hvd',\n",
       " 'over-all/jj',\n",
       " 'charge/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'deserves/vbz',\n",
       " 'the/at',\n",
       " 'praise/nn',\n",
       " 'and/cc',\n",
       " 'thanks/nns',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'City/nn-tl',\n",
       " 'of/in-tl',\n",
       " 'Atlanta/np-tl',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'for/in',\n",
       " 'the/at',\n",
       " 'manner/nn',\n",
       " 'in/in',\n",
       " 'which/wdt',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " 'was/bedz',\n",
       " 'conducted/vbn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'September-October/np',\n",
       " 'term/nn',\n",
       " 'jury/nn',\n",
       " 'had/hvd',\n",
       " 'been/ben',\n",
       " 'charged/vbn',\n",
       " 'by/in',\n",
       " 'Fulton/np-tl',\n",
       " 'Superior/jj-tl',\n",
       " 'Court/nn-tl',\n",
       " 'Judge/nn-tl',\n",
       " 'Durwood/np',\n",
       " 'Pye/np',\n",
       " 'to/to',\n",
       " 'investigate/vb',\n",
       " 'reports/nns',\n",
       " 'of/in',\n",
       " 'possible/jj',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'irregularities/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'hard-fought/jj',\n",
       " 'primary/nn',\n",
       " 'which/wdt',\n",
       " 'was/bedz',\n",
       " 'won/vbn',\n",
       " 'by/in',\n",
       " 'Mayor-nominate/nn-tl',\n",
       " 'Ivan/np',\n",
       " 'Allen/np',\n",
       " 'Jr./np',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'Only/rb',\n",
       " 'a/at',\n",
       " 'relative/jj',\n",
       " 'handful/nn',\n",
       " 'of/in',\n",
       " 'such/jj',\n",
       " 'reports/nns',\n",
       " 'was/bedz',\n",
       " 'received/vbn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'considering/in',\n",
       " 'the/at',\n",
       " 'widespread/jj',\n",
       " 'interest/nn',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'election/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'number/nn',\n",
       " 'of/in',\n",
       " 'voters/nns',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'size/nn',\n",
       " 'of/in',\n",
       " 'this/dt',\n",
       " 'city/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'did/dod',\n",
       " 'find/vb',\n",
       " 'that/cs',\n",
       " 'many/ap',\n",
       " 'of/in',\n",
       " \"Georgia's/np\",\n",
       " '$',\n",
       " 'registration/nn',\n",
       " 'and/cc',\n",
       " 'election/nn',\n",
       " 'laws/nns',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'are/ber',\n",
       " 'outmoded/jj',\n",
       " 'or/cc',\n",
       " 'inadequate/jj',\n",
       " 'and/cc',\n",
       " 'often/rb',\n",
       " 'ambiguous/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'recommended/vbd',\n",
       " 'that/cs',\n",
       " 'Fulton/np',\n",
       " 'legislators/nns',\n",
       " 'act/vb',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'to/to',\n",
       " 'have/hv',\n",
       " 'these/dts',\n",
       " 'laws/nns',\n",
       " 'studied/vbn',\n",
       " 'and/cc',\n",
       " 'revised/vbn',\n",
       " 'to/in',\n",
       " 'the/at',\n",
       " 'end/nn',\n",
       " 'of/in',\n",
       " 'modernizing/vbg',\n",
       " 'and/cc',\n",
       " 'improving/vbg',\n",
       " 'them/ppo',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'grand/jj',\n",
       " 'jury/nn',\n",
       " 'commented/vbd',\n",
       " 'on/in',\n",
       " 'a/at',\n",
       " 'number/nn',\n",
       " 'of/in',\n",
       " 'other/ap',\n",
       " 'topics/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'among/in',\n",
       " 'them/ppo',\n",
       " 'the/at',\n",
       " 'Atlanta/np',\n",
       " 'and/cc',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'purchasing/vbg',\n",
       " 'departments/nns',\n",
       " 'which/wdt',\n",
       " 'it/pps',\n",
       " 'said/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'are/ber',\n",
       " 'well/ql',\n",
       " 'operated/vbn',\n",
       " 'and/cc',\n",
       " 'follow/vb',\n",
       " 'generally/rb',\n",
       " 'accepted/vbn',\n",
       " 'practices/nns',\n",
       " 'which/wdt',\n",
       " 'inure/vb',\n",
       " 'to/in',\n",
       " 'the/at',\n",
       " 'best/jjt',\n",
       " 'interest/nn',\n",
       " 'of/in',\n",
       " 'both/abx',\n",
       " 'governments/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Merger/nn-hl',\n",
       " 'proposed/vbn-hl',\n",
       " 'However/wrb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'believes/vbz',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'these/dts',\n",
       " 'two/cd',\n",
       " 'offices/nns',\n",
       " 'should/md',\n",
       " 'be/be',\n",
       " 'combined/vbn',\n",
       " 'to/to',\n",
       " 'achieve/vb',\n",
       " 'greater/jjr',\n",
       " 'efficiency/nn',\n",
       " 'and/cc',\n",
       " 'reduce/vb',\n",
       " 'the/at',\n",
       " 'cost/nn',\n",
       " 'of/in',\n",
       " 'administration/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'City/nn-tl',\n",
       " 'Purchasing/vbg-tl',\n",
       " 'Department/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'is/bez',\n",
       " 'lacking/vbg',\n",
       " 'in/in',\n",
       " 'experienced/vbn',\n",
       " 'clerical/jj',\n",
       " 'personnel/nns',\n",
       " 'as/cs',\n",
       " 'a/at',\n",
       " 'result/nn',\n",
       " 'of/in',\n",
       " 'city/nn',\n",
       " 'personnel/nns',\n",
       " 'policies/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'urged/vbd',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'city/nn',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'take/vb',\n",
       " 'steps/nns',\n",
       " 'to/to',\n",
       " 'remedy/vb',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'this/dt',\n",
       " 'problem/nn',\n",
       " './',\n",
       " '.',\n",
       " 'Implementation/nn',\n",
       " 'of/in',\n",
       " \"Georgia's/np\",\n",
       " '$',\n",
       " 'automobile/nn',\n",
       " 'title/nn',\n",
       " 'law/nn',\n",
       " 'was/bedz',\n",
       " 'also/rb',\n",
       " 'recommended/vbn',\n",
       " 'by/in',\n",
       " 'the/at',\n",
       " 'outgoing/jj',\n",
       " 'jury/nn',\n",
       " './',\n",
       " '.',\n",
       " 'It/pps',\n",
       " 'urged/vbd',\n",
       " 'that/cs',\n",
       " 'the/at',\n",
       " 'next/ap',\n",
       " 'Legislature/nn-tl',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'provide/vb',\n",
       " 'enabling/vbg',\n",
       " 'funds/nns',\n",
       " 'and/cc',\n",
       " 're-set/vb',\n",
       " 'the/at',\n",
       " 'effective/jj',\n",
       " 'date/nn',\n",
       " 'so/cs',\n",
       " 'that/cs',\n",
       " 'an/at',\n",
       " 'orderly/jj',\n",
       " 'implementation/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'law/nn',\n",
       " 'may/md',\n",
       " 'be/be',\n",
       " 'effected/vbn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'grand/jj',\n",
       " 'jury/nn',\n",
       " 'took/vbd',\n",
       " 'a/at',\n",
       " 'swipe/nn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'State/nn-tl',\n",
       " 'Welfare/nn-tl',\n",
       " \"Department's/nn\",\n",
       " '$',\n",
       " '-tl',\n",
       " 'handling/nn',\n",
       " 'of/in',\n",
       " 'federal/jj',\n",
       " 'funds/nns',\n",
       " 'granted/vbn',\n",
       " 'for/in',\n",
       " 'child/nn',\n",
       " 'welfare/nn',\n",
       " 'services/nns',\n",
       " 'in/in',\n",
       " 'foster/jj',\n",
       " 'homes/nns',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'This/dt',\n",
       " 'is/bez',\n",
       " 'one/cd',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'major/jj',\n",
       " 'items/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'general/jj',\n",
       " 'assistance/nn',\n",
       " 'program/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'but/cc',\n",
       " 'the/at',\n",
       " 'State/nn-tl',\n",
       " 'Welfare/nn-tl',\n",
       " 'Department/nn-tl',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'has/hvz',\n",
       " 'seen/vbn',\n",
       " 'fit/jj',\n",
       " 'to/to',\n",
       " 'distribute/vb',\n",
       " 'these/dts',\n",
       " 'funds/nns',\n",
       " 'through/in',\n",
       " 'the/at',\n",
       " 'welfare/nn',\n",
       " 'departments/nns',\n",
       " 'of/in',\n",
       " 'all/abn',\n",
       " 'the/at',\n",
       " 'counties/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'state/nn',\n",
       " 'with/in',\n",
       " 'the/at',\n",
       " 'exception/nn',\n",
       " 'of/in',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'which/wdt',\n",
       " 'receives/vbz',\n",
       " 'none/pn',\n",
       " 'of/in',\n",
       " 'this/dt',\n",
       " 'money/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jurors/nns',\n",
       " 'said/vbd',\n",
       " 'they/ppss',\n",
       " 'realize/vb',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'a/at',\n",
       " 'proportionate/jj',\n",
       " 'distribution/nn',\n",
       " 'of/in',\n",
       " 'these/dts',\n",
       " 'funds/nns',\n",
       " 'might/md',\n",
       " 'disable/vb',\n",
       " 'this/dt',\n",
       " 'program/nn',\n",
       " 'in/in',\n",
       " 'our/pp',\n",
       " '$',\n",
       " 'less/ql',\n",
       " 'populous/jj',\n",
       " 'counties/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Nevertheless/rb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'we/ppss',\n",
       " 'feel/vb',\n",
       " 'that/cs',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'future/nn',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'should/md',\n",
       " 'receive/vb',\n",
       " 'some/dti',\n",
       " 'portion/nn',\n",
       " 'of/in',\n",
       " 'these/dts',\n",
       " 'available/jj',\n",
       " 'funds/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jurors/nns',\n",
       " 'said/vbd',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'Failure/nn',\n",
       " 'to/to',\n",
       " 'do/do',\n",
       " 'this/dt',\n",
       " 'will/md',\n",
       " 'continue/vb',\n",
       " 'to/to',\n",
       " 'place/vb',\n",
       " 'a/at',\n",
       " 'disproportionate/jj',\n",
       " 'burden/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'on/in',\n",
       " 'Fulton/np',\n",
       " 'taxpayers/nns',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'also/rb',\n",
       " 'commented/vbd',\n",
       " 'on/in',\n",
       " 'the/at',\n",
       " 'Fulton/np',\n",
       " \"ordinary's/nn\",\n",
       " '$',\n",
       " 'court/nn',\n",
       " 'which/wdt',\n",
       " 'has/hvz',\n",
       " 'been/ben',\n",
       " 'under/in',\n",
       " 'fire/nn',\n",
       " 'for/in',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'practices/nns',\n",
       " 'in/in',\n",
       " 'the/at',\n",
       " 'appointment/nn',\n",
       " 'of/in',\n",
       " 'appraisers/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'guardians/nns',\n",
       " 'and/cc',\n",
       " 'administrators/nns',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'awarding/nn',\n",
       " 'of/in',\n",
       " 'fees/nns',\n",
       " 'and/cc',\n",
       " 'compensation/nn',\n",
       " './',\n",
       " '.',\n",
       " 'Wards/nns-hl',\n",
       " 'protected/vbn-hl',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " 'it/pps',\n",
       " 'found/vbd',\n",
       " 'the/at',\n",
       " 'court/nn',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'has/hvz',\n",
       " 'incorporated/vbn',\n",
       " 'into/in',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'operating/vbg',\n",
       " 'procedures/nns',\n",
       " 'the/at',\n",
       " 'recommendations/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'of/in',\n",
       " 'two/cd',\n",
       " 'previous/jj',\n",
       " 'grand/jj',\n",
       " 'juries/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Atlanta/np-tl',\n",
       " 'Bar/nn-tl',\n",
       " 'Association/nn-tl',\n",
       " 'and/cc',\n",
       " 'an/at',\n",
       " 'interim/nn',\n",
       " 'citizens/nns',\n",
       " 'committee/nn',\n",
       " './',\n",
       " '.',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'These/dts',\n",
       " 'actions/nns',\n",
       " 'should/md',\n",
       " 'serve/vb',\n",
       " 'to/to',\n",
       " 'protect/vb',\n",
       " 'in/in',\n",
       " 'fact/nn',\n",
       " 'and/cc',\n",
       " 'in/in',\n",
       " 'effect/nn',\n",
       " 'the/at',\n",
       " \"court's/nn\",\n",
       " '$',\n",
       " 'wards/nns',\n",
       " 'from/in',\n",
       " 'undue/jj',\n",
       " 'costs/nns',\n",
       " 'and/cc',\n",
       " 'its/pp',\n",
       " '$',\n",
       " 'appointed/vbn',\n",
       " 'and/cc',\n",
       " 'elected/vbn',\n",
       " 'servants/nns',\n",
       " 'from/in',\n",
       " 'unmeritorious/jj',\n",
       " 'criticisms/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'said/vbd',\n",
       " './',\n",
       " '.',\n",
       " 'Regarding/in',\n",
       " \"Atlanta's/np\",\n",
       " '$',\n",
       " 'new/jj',\n",
       " 'multi-million-dollar/jj',\n",
       " 'airport/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'recommended/vbd',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'that/cs',\n",
       " 'when/wrb',\n",
       " 'the/at',\n",
       " 'new/jj',\n",
       " 'management/nn',\n",
       " 'takes/vbz',\n",
       " 'charge/nn',\n",
       " 'Jan./np',\n",
       " '1/cd',\n",
       " 'the/at',\n",
       " 'airport/nn',\n",
       " 'be/be',\n",
       " 'operated/vbn',\n",
       " 'in/in',\n",
       " 'a/at',\n",
       " 'manner/nn',\n",
       " 'that/wps',\n",
       " 'will/md',\n",
       " 'eliminate/vb',\n",
       " 'political/jj',\n",
       " 'influences/nns',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'did/dod',\n",
       " 'not/',\n",
       " '*',\n",
       " 'elaborate/vb',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'but/cc',\n",
       " 'it/pps',\n",
       " 'added/vbd',\n",
       " 'that/cs',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'there/ex',\n",
       " 'should/md',\n",
       " 'be/be',\n",
       " 'periodic/jj',\n",
       " 'surveillance/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'pricing/vbg',\n",
       " 'practices/nns',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'concessionaires/nns',\n",
       " 'for/in',\n",
       " 'the/at',\n",
       " 'purpose/nn',\n",
       " 'of/in',\n",
       " 'keeping/vbg',\n",
       " 'the/at',\n",
       " 'prices/nns',\n",
       " 'reasonable/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " 'Ask/vb-hl',\n",
       " 'jail/nn-hl',\n",
       " 'deputies/nns-hl',\n",
       " 'On/in',\n",
       " 'other/ap',\n",
       " 'matters/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'jury/nn',\n",
       " 'recommended/vbd',\n",
       " 'that/cs',\n",
       " ':',\n",
       " '/',\n",
       " ':',\n",
       " '(',\n",
       " '/',\n",
       " '(',\n",
       " '1/cd',\n",
       " ')',\n",
       " '/',\n",
       " ')',\n",
       " 'Four/cd',\n",
       " 'additional/jj',\n",
       " 'deputies/nns',\n",
       " 'be/be',\n",
       " 'employed/vbn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'County/nn-tl',\n",
       " 'Jail/nn-tl',\n",
       " 'and/cc',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'a/at',\n",
       " 'doctor/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'medical/jj',\n",
       " 'intern/nn',\n",
       " 'or/cc',\n",
       " 'extern/nn',\n",
       " 'be/be',\n",
       " 'employed/vbn',\n",
       " 'for/in',\n",
       " 'night/nn',\n",
       " 'and/cc',\n",
       " 'weekend/nn',\n",
       " 'duty/nn',\n",
       " 'at/in',\n",
       " 'the/at',\n",
       " 'jail/nn',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " './',\n",
       " '.',\n",
       " '(',\n",
       " '/',\n",
       " '(',\n",
       " '2/cd',\n",
       " ')',\n",
       " '/',\n",
       " ')',\n",
       " 'Fulton/np',\n",
       " 'legislators/nns',\n",
       " '``',\n",
       " '/',\n",
       " '``',\n",
       " 'work/vb',\n",
       " 'with/in',\n",
       " 'city/nn',\n",
       " 'officials/nns',\n",
       " 'to/to',\n",
       " 'pass/vb',\n",
       " 'enabling/vbg',\n",
       " 'legislation/nn',\n",
       " 'that/wps',\n",
       " 'will/md',\n",
       " 'permit/vb',\n",
       " 'the/at',\n",
       " 'establishment/nn',\n",
       " 'of/in',\n",
       " 'a/at',\n",
       " 'fair/jj',\n",
       " 'and/cc',\n",
       " 'equitable/jj',\n",
       " '``',\n",
       " '/',\n",
       " \"''\",\n",
       " 'pension/nn',\n",
       " 'plan/nn',\n",
       " 'for/in',\n",
       " 'city/nn',\n",
       " 'employes/nns',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'jury/nn',\n",
       " 'praised/vbd',\n",
       " 'the/at',\n",
       " 'administration/nn',\n",
       " 'and/cc',\n",
       " 'operation/nn',\n",
       " 'of/in',\n",
       " 'the/at',\n",
       " 'Atlanta/np-tl',\n",
       " 'Police/nns-tl',\n",
       " 'Department/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'Tax/nn-tl',\n",
       " \"Commissioner's/nn\",\n",
       " '$',\n",
       " '-tl',\n",
       " 'Office/nn-tl',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'the/at',\n",
       " 'Bellwood/np',\n",
       " 'and/cc',\n",
       " 'Alpharetta/np',\n",
       " 'prison/nn',\n",
       " 'farms/nns',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'Grady/np-tl',\n",
       " 'Hospital/nn-tl',\n",
       " 'and/cc',\n",
       " 'the/at',\n",
       " 'Fulton/np-tl',\n",
       " 'Health/nn-tl',\n",
       " 'Department/nn-tl',\n",
       " './',\n",
       " '.',\n",
       " 'Mayor/nn-tl',\n",
       " 'William/np',\n",
       " 'B./np',\n",
       " 'Hartsfield/np',\n",
       " 'filed/vbd',\n",
       " 'suit/nn',\n",
       " 'for/in',\n",
       " 'divorce/nn',\n",
       " 'from/in',\n",
       " 'his/pp',\n",
       " '$',\n",
       " 'wife/nn',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'Pearl/np',\n",
       " 'Williams/np',\n",
       " 'Hartsfield/np',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " 'in/in',\n",
       " 'Fulton/np-tl',\n",
       " 'Superior/jj-tl',\n",
       " 'Court/nn-tl',\n",
       " 'Friday/nr',\n",
       " './',\n",
       " '.',\n",
       " 'His/pp',\n",
       " '$',\n",
       " 'petition/nn',\n",
       " 'charged/vbd',\n",
       " 'mental/jj',\n",
       " 'cruelty/nn',\n",
       " './',\n",
       " '.',\n",
       " 'The/at',\n",
       " 'couple/nn',\n",
       " 'was/bedz',\n",
       " 'married/vbn',\n",
       " 'Aug./np',\n",
       " '2/cd',\n",
       " ',',\n",
       " '/',\n",
       " ',',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokens = nltk.word_tokenize(brown.raw())\n",
    "print(\"Number of Tokens:\", len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b74a5b",
   "metadata": {},
   "source": [
    " #### Categories in Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b438d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661281c9",
   "metadata": {},
   "source": [
    "#### Size of Word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e3fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Word Tokens:  1161192\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Word Tokens: \", len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8f917",
   "metadata": {},
   "source": [
    "#### Size of Word Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed38716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of word types:  56057\n"
     ]
    }
   ],
   "source": [
    "word_types = len(set(brown.words()))\n",
    "print(\"The size of word types: \",word_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a929b4",
   "metadata": {},
   "source": [
    "#### Finding the size of the category \"government\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c106f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the category 'government':  70117\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the category 'government': \",len(brown.words(categories='government')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e3c21",
   "metadata": {},
   "source": [
    "#### Listing the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b08b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fdist1 = FreqDist(brown.words())\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b6300",
   "metadata": {},
   "source": [
    "#### Counting the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17bf505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences:\", len(brown.sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757be68e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9caefd",
   "metadata": {},
   "source": [
    "Q2 ) Explore the corpora available in NLTK (any two)\n",
    "\n",
    "    • Raw corpus\n",
    "    • POS tagged\n",
    "    • Parsed\n",
    "    • Multilingual aligned\n",
    "    • Spoken language\n",
    "    • Semantic tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fcf3c",
   "metadata": {},
   "source": [
    "#### Raw Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee03cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61a52c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaaf7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  2621613\n",
      "Number of sentences :  98503\n",
      "Number of paragraphs :  47887\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words : \", len(gutenberg.words()))\n",
    "print(\"Number of sentences : \", len(gutenberg.sents()))\n",
    "print(\"Number of paragraphs : \", len(gutenberg.paras()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d456d0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdafb7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d92b54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']], ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f43d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word types: 51156\n"
     ]
    }
   ],
   "source": [
    "word_types = len(set(gutenberg.words()))\n",
    "print(\"Size of word types:\", word_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c8c8610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
      "5               25               26        austen-emma.txt\n",
      "5               26               17        austen-persuasion.txt\n",
      "5               28               22        austen-sense.txt\n",
      "4               34               79        bible-kjv.txt\n",
      "5               19               5        blake-poems.txt\n",
      "4               19               14        bryant-stories.txt\n",
      "4               18               12        burgess-busterbrown.txt\n",
      "4               20               13        carroll-alice.txt\n",
      "5               20               12        chesterton-ball.txt\n",
      "5               23               11        chesterton-brown.txt\n",
      "5               19               11        chesterton-thursday.txt\n",
      "4               21               25        edgeworth-parents.txt\n",
      "5               26               15        melville-moby_dick.txt\n",
      "5               52               11        milton-paradise.txt\n",
      "4               12               9        shakespeare-caesar.txt\n",
      "4               12               8        shakespeare-hamlet.txt\n",
      "4               12               7        shakespeare-macbeth.txt\n",
      "5               36               12        whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
    "for fileid in gutenberg.fileids(): \n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d3faf",
   "metadata": {},
   "source": [
    "#### Parsed Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9bbb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634b0959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d3bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1bdaa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'], ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1fb4cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c9073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent tokens: [(',', 4885), ('the', 4045), ('.', 3828), ('of', 2319), ('to', 2164), ('a', 1878), ('in', 1572), ('and', 1511), ('*-1', 1123), ('0', 1099)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(treebank.words())\n",
    "top_tokens = fdist.most_common(10)\n",
    "print(\"Most frequent tokens:\", top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "808c8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in treebank corpus :  199\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of files in treebank corpus : \",len(treebank.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9006c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
      "21               16               1        wsj_0001.mrg\n",
      "23               27               1        wsj_0002.mrg\n",
      "29               26               2        wsj_0003.mrg\n",
      "24               24               2        wsj_0004.mrg\n",
      "25               19               1        wsj_0005.mrg\n",
      "28               24               1        wsj_0006.mrg\n",
      "23               19               1        wsj_0007.mrg\n",
      "26               23               2        wsj_0008.mrg\n",
      "23               20               2        wsj_0009.mrg\n",
      "24               22               2        wsj_0010.mrg\n",
      "24               26               2        wsj_0011.mrg\n",
      "26               28               2        wsj_0012.mrg\n",
      "28               31               2        wsj_0013.mrg\n",
      "22               37               1        wsj_0014.mrg\n",
      "26               26               2        wsj_0015.mrg\n",
      "25               22               2        wsj_0016.mrg\n",
      "28               26               2        wsj_0017.mrg\n",
      "27               27               2        wsj_0018.mrg\n",
      "24               15               2        wsj_0019.mrg\n",
      "30               31               2        wsj_0020.mrg\n",
      "29               25               2        wsj_0021.mrg\n",
      "25               21               2        wsj_0022.mrg\n",
      "25               24               1        wsj_0023.mrg\n",
      "29               22               2        wsj_0024.mrg\n",
      "26               29               2        wsj_0025.mrg\n",
      "30               32               2        wsj_0026.mrg\n",
      "27               23               2        wsj_0027.mrg\n",
      "22               23               1        wsj_0028.mrg\n",
      "24               27               2        wsj_0029.mrg\n",
      "23               27               1        wsj_0030.mrg\n",
      "32               40               1        wsj_0031.mrg\n",
      "26               24               2        wsj_0032.mrg\n",
      "24               24               2        wsj_0033.mrg\n",
      "27               24               2        wsj_0034.mrg\n",
      "30               28               2        wsj_0035.mrg\n",
      "27               21               3        wsj_0036.mrg\n",
      "28               28               2        wsj_0037.mrg\n",
      "30               24               2        wsj_0038.mrg\n",
      "27               25               2        wsj_0039.mrg\n",
      "25               27               2        wsj_0040.mrg\n",
      "27               25               3        wsj_0041.mrg\n",
      "23               24               2        wsj_0042.mrg\n",
      "25               26               3        wsj_0043.mrg\n",
      "28               23               3        wsj_0044.mrg\n",
      "28               27               3        wsj_0045.mrg\n",
      "23               19               1        wsj_0046.mrg\n",
      "31               29               2        wsj_0047.mrg\n",
      "27               25               2        wsj_0048.mrg\n",
      "29               27               3        wsj_0049.mrg\n",
      "31               21               1        wsj_0050.mrg\n",
      "28               25               2        wsj_0051.mrg\n",
      "25               15               1        wsj_0052.mrg\n",
      "25               30               2        wsj_0053.mrg\n",
      "28               23               1        wsj_0054.mrg\n",
      "29               30               1        wsj_0055.mrg\n",
      "19               9               1        wsj_0056.mrg\n",
      "26               25               2        wsj_0057.mrg\n",
      "27               25               1        wsj_0058.mrg\n",
      "27               32               2        wsj_0059.mrg\n",
      "29               27               2        wsj_0060.mrg\n",
      "25               18               1        wsj_0061.mrg\n",
      "28               23               2        wsj_0062.mrg\n",
      "29               30               2        wsj_0063.mrg\n",
      "28               25               2        wsj_0064.mrg\n",
      "27               24               1        wsj_0065.mrg\n",
      "22               24               1        wsj_0066.mrg\n",
      "30               32               2        wsj_0067.mrg\n",
      "23               19               2        wsj_0068.mrg\n",
      "25               23               1        wsj_0069.mrg\n",
      "31               33               1        wsj_0070.mrg\n",
      "27               29               3        wsj_0071.mrg\n",
      "27               29               2        wsj_0072.mrg\n",
      "29               26               2        wsj_0073.mrg\n",
      "26               18               2        wsj_0074.mrg\n",
      "29               30               2        wsj_0075.mrg\n",
      "25               18               1        wsj_0076.mrg\n",
      "26               17               2        wsj_0077.mrg\n",
      "24               28               1        wsj_0078.mrg\n",
      "24               16               1        wsj_0079.mrg\n",
      "29               28               2        wsj_0080.mrg\n",
      "28               32               2        wsj_0081.mrg\n",
      "28               26               2        wsj_0082.mrg\n",
      "25               26               3        wsj_0083.mrg\n",
      "24               21               2        wsj_0084.mrg\n",
      "27               24               2        wsj_0085.mrg\n",
      "32               22               2        wsj_0086.mrg\n",
      "26               18               2        wsj_0087.mrg\n",
      "30               32               3        wsj_0088.mrg\n",
      "27               27               3        wsj_0089.mrg\n",
      "26               27               3        wsj_0090.mrg\n",
      "30               28               2        wsj_0091.mrg\n",
      "29               27               2        wsj_0092.mrg\n",
      "28               27               2        wsj_0093.mrg\n",
      "28               22               2        wsj_0094.mrg\n",
      "28               26               2        wsj_0095.mrg\n",
      "25               38               3        wsj_0096.mrg\n",
      "28               26               2        wsj_0097.mrg\n",
      "28               33               2        wsj_0098.mrg\n",
      "22               22               2        wsj_0099.mrg\n",
      "27               23               2        wsj_0100.mrg\n",
      "27               42               2        wsj_0101.mrg\n",
      "27               20               2        wsj_0102.mrg\n",
      "29               29               2        wsj_0103.mrg\n",
      "20               16               1        wsj_0104.mrg\n",
      "29               28               2        wsj_0105.mrg\n",
      "27               25               2        wsj_0106.mrg\n",
      "34               40               2        wsj_0107.mrg\n",
      "29               23               3        wsj_0108.mrg\n",
      "28               25               2        wsj_0109.mrg\n",
      "20               22               3        wsj_0110.mrg\n",
      "29               25               2        wsj_0111.mrg\n",
      "31               31               3        wsj_0112.mrg\n",
      "25               26               2        wsj_0113.mrg\n",
      "26               24               2        wsj_0114.mrg\n",
      "30               28               2        wsj_0115.mrg\n",
      "27               28               3        wsj_0116.mrg\n",
      "28               27               2        wsj_0117.mrg\n",
      "27               26               4        wsj_0118.mrg\n",
      "27               31               2        wsj_0119.mrg\n",
      "25               21               2        wsj_0120.mrg\n",
      "25               23               3        wsj_0121.mrg\n",
      "30               30               1        wsj_0122.mrg\n",
      "30               26               2        wsj_0123.mrg\n",
      "25               23               2        wsj_0124.mrg\n",
      "23               20               3        wsj_0125.mrg\n",
      "27               22               2        wsj_0126.mrg\n",
      "25               28               2        wsj_0127.mrg\n",
      "27               28               3        wsj_0128.mrg\n",
      "30               31               2        wsj_0129.mrg\n",
      "31               28               2        wsj_0130.mrg\n",
      "22               27               1        wsj_0131.mrg\n",
      "26               27               2        wsj_0132.mrg\n",
      "28               25               1        wsj_0133.mrg\n",
      "29               26               2        wsj_0134.mrg\n",
      "28               25               2        wsj_0135.mrg\n",
      "26               30               2        wsj_0136.mrg\n",
      "27               21               2        wsj_0137.mrg\n",
      "21               26               2        wsj_0138.mrg\n",
      "26               16               1        wsj_0139.mrg\n",
      "26               29               1        wsj_0140.mrg\n",
      "24               23               2        wsj_0141.mrg\n",
      "25               24               3        wsj_0142.mrg\n",
      "24               32               1        wsj_0143.mrg\n",
      "24               29               2        wsj_0144.mrg\n",
      "26               27               2        wsj_0145.mrg\n",
      "31               30               2        wsj_0146.mrg\n",
      "27               32               2        wsj_0147.mrg\n",
      "26               29               3        wsj_0148.mrg\n",
      "29               26               2        wsj_0149.mrg\n",
      "25               20               2        wsj_0150.mrg\n",
      "29               31               2        wsj_0151.mrg\n",
      "24               23               2        wsj_0152.mrg\n",
      "24               26               2        wsj_0153.mrg\n",
      "26               29               2        wsj_0154.mrg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29               26               3        wsj_0155.mrg\n",
      "32               29               2        wsj_0156.mrg\n",
      "25               29               2        wsj_0157.mrg\n",
      "26               25               2        wsj_0158.mrg\n",
      "28               25               2        wsj_0159.mrg\n",
      "27               32               2        wsj_0160.mrg\n",
      "30               32               2        wsj_0161.mrg\n",
      "27               25               2        wsj_0162.mrg\n",
      "28               25               2        wsj_0163.mrg\n",
      "27               27               2        wsj_0164.mrg\n",
      "27               29               2        wsj_0165.mrg\n",
      "28               32               2        wsj_0166.mrg\n",
      "25               24               2        wsj_0167.mrg\n",
      "24               30               2        wsj_0168.mrg\n",
      "24               25               2        wsj_0169.mrg\n",
      "29               22               2        wsj_0170.mrg\n",
      "26               23               2        wsj_0171.mrg\n",
      "28               26               2        wsj_0172.mrg\n",
      "26               29               2        wsj_0173.mrg\n",
      "29               25               2        wsj_0174.mrg\n",
      "26               24               2        wsj_0175.mrg\n",
      "28               33               2        wsj_0176.mrg\n",
      "26               21               2        wsj_0177.mrg\n",
      "25               20               2        wsj_0178.mrg\n",
      "26               23               2        wsj_0179.mrg\n",
      "28               26               2        wsj_0180.mrg\n",
      "27               30               2        wsj_0181.mrg\n",
      "26               24               2        wsj_0182.mrg\n",
      "26               29               2        wsj_0183.mrg\n",
      "28               24               2        wsj_0184.mrg\n",
      "23               13               1        wsj_0185.mrg\n",
      "28               26               2        wsj_0186.mrg\n",
      "26               26               2        wsj_0187.mrg\n",
      "28               30               2        wsj_0188.mrg\n",
      "23               25               2        wsj_0189.mrg\n",
      "25               17               1        wsj_0190.mrg\n",
      "26               27               1        wsj_0191.mrg\n",
      "27               25               3        wsj_0192.mrg\n",
      "28               26               1        wsj_0193.mrg\n",
      "27               28               2        wsj_0194.mrg\n",
      "28               31               1        wsj_0195.mrg\n",
      "25               44               1        wsj_0196.mrg\n",
      "29               54               1        wsj_0197.mrg\n",
      "29               32               2        wsj_0198.mrg\n",
      "28               15               1        wsj_0199.mrg\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
    "for fileid in treebank.fileids(): \n",
    "    num_chars = len(treebank.raw(fileid))\n",
    "    num_words = len(treebank.words(fileid))\n",
    "    num_sents = len(treebank.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in treebank.words(fileid)))\n",
    "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48a346",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f07e90",
   "metadata": {},
   "source": [
    "Q3 ) Create a text corpus with a minimum of 200 words (unique content). Implement the following text processing\n",
    "\n",
    "    • Word segmentation\n",
    "    • Sentence segmentation\n",
    "    • Convert to Lowercase\n",
    "    • Stop words removal\n",
    "    • Stemming\n",
    "    • Lemmatization\n",
    "    • Part of speech tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19737d6f",
   "metadata": {},
   "source": [
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01ebd42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'to',\n",
       " 'test',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'various',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " '.',\n",
       " 'This',\n",
       " 'text',\n",
       " 'covers',\n",
       " 'and',\n",
       " 'uses',\n",
       " 'all',\n",
       " 'the',\n",
       " 'punctuation',\n",
       " 'marks',\n",
       " '!',\n",
       " 'Although',\n",
       " 'this',\n",
       " 'has',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'questionable',\n",
       " 'approach',\n",
       " 'in',\n",
       " 'sentence',\n",
       " 'making',\n",
       " ',',\n",
       " 'I',\n",
       " 'have',\n",
       " 'succeeded',\n",
       " 'in',\n",
       " 'including',\n",
       " 'all',\n",
       " 'the',\n",
       " 'punctuations',\n",
       " '.',\n",
       " 'Is',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'great',\n",
       " '?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"This is a sample text to test and implement various forms of text processing. This text covers and uses all the punctuation marks! Although this has turned out to be a very questionable approach in sentence making, I have succeeded in including all the punctuations. Isn't this great?\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca22e7c",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce738577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a sample text to test and implement various forms of text processing.',\n",
       " 'This text covers and uses all the punctuation marks!',\n",
       " 'Although this has turned out to be a very questionable approach in sentence making, I have succeeded in including all the punctuations.',\n",
       " \"Isn't this great?\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32c503",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1503882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample text to test and implement various forms of text processing. this text covers and uses all the punctuation marks! although this has turned out to be a very questionable approach in sentence making, i have succeeded in including all the punctuations. isn't this great?\n"
     ]
    }
   ],
   "source": [
    "lower_text = text.lower()\n",
    "print (lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661446f5",
   "metadata": {},
   "source": [
    "#### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9de78710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c27c735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words left after stop words removal :  ['sample', 'text', 'test', 'implement', 'various', 'forms', 'text', 'processing', '.', 'text', 'covers', 'uses', 'punctuation', 'marks', '!', 'although', 'turned', 'questionable', 'approach', 'sentence', 'making', ',', 'succeeded', 'including', 'punctuations', '.', \"n't\", 'great', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "word_tokens = word_tokenize(text.lower())\n",
    "filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
    "print(\"Words left after stop words removal : \",filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fbcf4",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc7e75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'a', 'sampl', 'text', 'to', 'test', 'and', 'implement', 'variou', 'form', 'of', 'text', 'process', '.', 'thi', 'text', 'cover', 'and', 'use', 'all', 'the', 'punctuat', 'mark', '!', 'although', 'thi', 'ha', 'turn', 'out', 'to', 'be', 'a', 'veri', 'question', 'approach', 'in', 'sentenc', 'make', ',', 'i', 'have', 'succeed', 'in', 'includ', 'all', 'the', 'punctuat', '.', 'is', \"n't\", 'thi', 'great', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0259ffd",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96d3382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cd3c897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'to',\n",
       " 'test',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'various',\n",
       " 'form',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " '.',\n",
       " 'This',\n",
       " 'text',\n",
       " 'cover',\n",
       " 'and',\n",
       " 'us',\n",
       " 'all',\n",
       " 'the',\n",
       " 'punctuation',\n",
       " 'mark',\n",
       " '!',\n",
       " 'Although',\n",
       " 'this',\n",
       " 'ha',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'questionable',\n",
       " 'approach',\n",
       " 'in',\n",
       " 'sentence',\n",
       " 'making',\n",
       " ',',\n",
       " 'I',\n",
       " 'have',\n",
       " 'succeeded',\n",
       " 'in',\n",
       " 'including',\n",
       " 'all',\n",
       " 'the',\n",
       " 'punctuation',\n",
       " '.',\n",
       " 'Is',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'great',\n",
       " '?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12cc0e",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28ed510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\roshp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e517b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('sample', 'NN'), ('text', 'JJ'), ('test', 'NN'), ('implement', 'NN'), ('various', 'JJ'), ('forms', 'NNS'), ('text', 'IN'), ('processing', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('text', 'NN'), ('covers', 'VBZ'), ('uses', 'VBZ'), ('punctuation', 'NN'), ('marks', 'NNS'), ('!', '.')]\n",
      "[('Although', 'IN'), ('turned', 'VBN'), ('questionable', 'JJ'), ('approach', 'NN'), ('sentence', 'NN'), ('making', 'NN'), (',', ','), ('I', 'PRP'), ('succeeded', 'VBD'), ('including', 'VBG'), ('punctuations', 'NNS'), ('.', '.')]\n",
      "[('Is', 'VBZ'), (\"n't\", 'RB'), ('great', 'JJ'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized = sent_tokenize(text)\n",
    "for i in tokenized:\n",
    "\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    " \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    " \n",
    "    print(tagged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
